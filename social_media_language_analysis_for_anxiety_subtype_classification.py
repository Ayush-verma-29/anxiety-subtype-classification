# -*- coding: utf-8 -*-
"""Social Media Language Analysis for Anxiety Subtype Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YUpyAaX-Pk4ckui9WHY7SnKxgGAMr6O8
"""

# ================
# Cell 1: Install
# ================
!pip install praw python-dotenv pandas sentence-transformers scikit-learn imbalanced-learn matplotlib transformers datasets accelerate shap streamlit umap-learn

# =================================================================================
# Cell 2: Imports & basic settings
# =================================================================================
import os
import json
import time
import math
import random
import html
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from dotenv import load_dotenv
from tqdm.notebook import tqdm

# ML libs
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE

# Embeddings
from sentence_transformers import SentenceTransformer

# UMAP for visualization
import umap

# For optional HF fine-tune
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer

# Misc
tqdm.pandas()
random.seed(42)
np.random.seed(42)

# Verify Secrets Are Loaded

import os

print("CLIENT_ID exists:", "REDDIT_CLIENT_ID" in os.environ)
print("SECRET exists:", "REDDIT_SECRET" in os.environ)
print("USER_AGENT exists:", "REDDIT_USER_AGENT" in os.environ)

# =================================================================================
# Cell 3: Data collection using PRAW
# =================================================================================

import praw
import pandas as pd
import os

reddit = praw.Reddit(
    client_id=os.environ["REDDIT_CLIENT_ID"],
    client_secret=os.environ["REDDIT_SECRET"],
    user_agent=os.environ["REDDIT_USER_AGENT"]
)

def fetch_subreddit(subreddit, limit=2000):
    rows = []
    for post in reddit.subreddit(subreddit).new(limit=limit):
        rows.append({
            "post_id": post.id,
            "title": post.title,
            "body": post.selftext,
            "created_utc": post.created_utc,
            "subreddit": subreddit
        })
    return pd.DataFrame(rows)

subs = ["anxiety", "panicdisorder", "socialanxiety"]
frames = []

for s in subs:
    print("Fetching:", s)
    frames.append(fetch_subreddit(s, limit=2000))

df_raw = pd.concat(frames, ignore_index=True)

os.makedirs("data/raw", exist_ok=True)
df_raw.to_csv("data/raw/reddit_raw.csv", index=False)

print("Saved data/raw/reddit_raw.csv")

# =================================================================================
# Cell 4: Load data (either fetched or existing CSV)
# =================================================================================
# If you ran previous cell, df_raw exists. Otherwise load from file.
if 'df_raw' not in globals():
    # change path if your csv is named differently
    df_raw = pd.read_csv("data/raw/reddit_raw.csv")

print("Rows:", len(df_raw))

df_raw['created_utc']=pd.to_datetime(df_raw['created_utc'],unit='s')
df_raw['created_utc']=df_raw['created_utc'].dt.tz_localize('UTC')
df_raw.head(3)

!pip install emoji
!pip install unidecode

# =================================================================================
# Cell 5: Preprocessing functions
# =================================================================================
import emoji
from unidecode import unidecode

def clean_text(text):
    # robust cleaning for Reddit posts
    if not isinstance(text, str):
        return ""
    text = html.unescape(text)
    text = unidecode(text)
    text = emoji.demojize(text)  # :smile: -> ':smile:'
    text = text.lower()
    # remove urls
    text = re.sub(r'http\S+|www.\S+', ' ', text)
    # remove subreddit/user mentions
    text = re.sub(r'u\/\w+|\/u\/\w+|r\/\w+', ' ', text)
    # remove non alphabetic sequences (keep basic punctuation if needed)
    text = re.sub(r'[^0-9a-zA-Z\s\:\_]', ' ', text)
    # collapse whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def preprocess_df(df):
    df = df.copy()
    df['title'] = df.get('title', '').fillna('')
    df['body'] = df.get('body', '').fillna('')
    df['raw_text'] = (df['title'] + " " + df['body']).str.strip()
    df['clean_text'] = df['raw_text'].apply(clean_text)
    df['text_len'] = df['clean_text'].apply(lambda x: len(x.split()))
    return df

df = preprocess_df(df_raw)
os.makedirs("data/processed", exist_ok=True)
df.to_csv("data/processed/reddit_clean.csv", index=False)
print("Saved data/processed/reddit_clean.csv")
df.head(3)

# =================================================================================
# Cell 6: Weak supervision / keyword-based labeling (keeps unknown bucket)
# =================================================================================
# Simple, extensible keyword lists â€” expand them according to your domain knowledge
GAD_KEYWORDS = ["worry", "worrying", "cant stop worrying", "generalized anxiety", "constant worry", "g.a.d", "gad"]
PANIC_KEYWORDS = ["panic attack", "panic attacks", "palpitations", "heart racing", "choking", "derealization", "panic disorder", "panic_disorder"]
SOCIAL_KEYWORDS = ["social anxiety", "being judged", "public", "embarrass", "performance anxiety", "social_anxiety", "crowd", "meeting people", "speaking"]

def count_keywords(text, keywords):
    text = (text or "").lower()
    cnt = 0
    for kw in keywords:
        if kw in text:
            cnt += 1
    return cnt

def weak_label_df(df):
    df = df.copy()
    df['gad_kw_freq'] = df['clean_text'].apply(lambda x: count_keywords(x, GAD_KEYWORDS))
    df['panic_kw_freq'] = df['clean_text'].apply(lambda x: count_keywords(x, PANIC_KEYWORDS))
    df['social_kw_freq'] = df['clean_text'].apply(lambda x: count_keywords(x, SOCIAL_KEYWORDS))
    def assign_label(row):
        scores = {
            'generalized_anxiety': row['gad_kw_freq'],
            'panic_disorder': row['panic_kw_freq'],
            'social_anxiety': row['social_kw_freq']
        }
        # pick max, but require at least 1 hit and no tie
        max_label = max(scores, key=scores.get)
        vals = sorted(scores.values(), reverse=True)
        if vals[0] == 0:
            return 'unknown'
        if len(vals) > 1 and vals[0] == vals[1]:
            return 'unknown'
        return max_label
    df['subtype_label'] = df.apply(assign_label, axis=1)
    return df

df = weak_label_df(df)
df['subtype_label'].value_counts()
# Save labeled intermediate
df.to_csv("data/processed/reddit_labeled_weak.csv", index=False)
print("Saved data/processed/reddit_labeled_weak.csv")

# =================================================================================
# Cell 7 (FAST): Sentiment scoring using VADER (continuous [-1, 1])
# =================================================================================
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer

nltk.download("vader_lexicon")

sia = SentimentIntensityAnalyzer()

df["sentiment"] = df["clean_text"].apply(
    lambda x: sia.polarity_scores(x)["compound"]
)

# =================================================================================
# Cell 8: Create sentiment bins and clean unknown subtypes for downstream modeling
# =================================================================================
bins = [-1.1, -0.3, 0.3, 1.1]  # widened to include edge cases
labels = ['Negative','Neutral','Positive']
df['sentiment_bin'] = pd.cut(df['sentiment'], bins=bins, labels=labels)

# Option: drop 'unknown' in training set, keep them for prediction later
valid_subtypes = ['generalized_anxiety','panic_disorder','social_anxiety']
df_labeled = df[df['subtype_label'].isin(valid_subtypes)].reset_index(drop=True)
df_unlabeled = df[df['subtype_label']=='unknown'].reset_index(drop=True)

print("Labeled:", df_labeled.shape, "Unlabeled:", df_unlabeled.shape)
df_labeled['subtype_label'].value_counts()

# =================================================================================
# Compute SBERT embeddings for FULL dataset (df)
# =================================================================================
from sentence_transformers import SentenceTransformer
import numpy as np
import os

sbert = SentenceTransformer("all-MiniLM-L6-v2")

all_texts = df["clean_text"].tolist()

emb_all = sbert.encode(
    all_texts,
    batch_size=128,
    show_progress_bar=True
)

os.makedirs("features", exist_ok=True)
np.save("features/sbert_embeddings_all.npy", emb_all)

print("Embeddings shape:", emb_all.shape)  # MUST be (2988, 768)

df["text_len"] = df["clean_text"].str.len()

train_df = df[df["text_len"] < 200]
test_df  = df[df["text_len"] >= 200]

from sklearn.metrics import (
    accuracy_score, precision_recall_fscore_support, classification_report
)
import pandas as pd
import joblib

from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression


# =================================================================================
# Keyword feature column list (DEFINE ONCE)
# =================================================================================
kw_cols = ["gad_kw_freq", "panic_kw_freq", "social_kw_freq"]

# Load embeddings
emb_all = np.load("features/sbert_embeddings_all.npy")

# Use only labeled data
mask = df["subtype_label"] != "unknown"

# Embeddings (already computed from df)
X_emb_labeled = emb_all[mask]

# Labels
y = df.loc[mask, "subtype_label"].values

# Keyword + sentiment
kwf = df.loc[mask, kw_cols].values.astype(np.float32)
sent = df.loc[mask, "sentiment"].values.reshape(-1, 1).astype(np.float32)

# Final feature matrix
#X = np.hstack([X_emb_labeled, kwf, sent])
X = np.hstack([X_emb_labeled])

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# X_train = emb_all[train_df.index]
# y_train = train_df["subtype_label"]

# X_test = emb_all[test_df.index]
# y_test = test_df["subtype_label"]

clf = LogisticRegression(max_iter=5000, class_weight="balanced", n_jobs=-1)
clf.fit(X_train, y_train)

preds_lr = clf.predict(X_test)
print(classification_report(y_test, preds_lr))

from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC


# =================================================================================
# Keyword feature column list (DEFINE ONCE)
# =================================================================================
kw_cols = ["gad_kw_freq", "panic_kw_freq", "social_kw_freq"]

# Load embeddings
emb_all = np.load("features/sbert_embeddings_all.npy")

# Use only labeled data
mask = df["subtype_label"] != "unknown"

# Embeddings (already computed from df)
X_emb_labeled = emb_all[mask]

# Labels
y = df.loc[mask, "subtype_label"].values

# Keyword + sentiment
kwf = df.loc[mask, kw_cols].values.astype(np.float32)
sent = df.loc[mask, "sentiment"].values.reshape(-1, 1).astype(np.float32)

# Final feature matrix
#X = np.hstack([X_emb_labeled, kwf, sent])
X = np.hstack([X_emb_labeled])

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# X_train = emb_all[train_df.index]
# y_train = train_df["subtype_label"]

# X_test = emb_all[test_df.index]
# y_test = test_df["subtype_label"]

svm = LinearSVC(
    class_weight="balanced",
    max_iter=5000
)

svm.fit(X_train, y_train)
preds_svm = svm.predict(X_test)

print("Linear SVM")
print(classification_report(y_test, preds_svm))

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier


# =================================================================================
# Keyword feature column list (DEFINE ONCE)
# =================================================================================
kw_cols = ["gad_kw_freq", "panic_kw_freq", "social_kw_freq"]

# Load embeddings
emb_all = np.load("features/sbert_embeddings_all.npy")

# Use only labeled data
mask = df["subtype_label"] != "unknown"

# Embeddings (already computed from df)
X_emb_labeled = emb_all[mask]

# Labels
y = df.loc[mask, "subtype_label"].values

# Keyword + sentiment
kwf = df.loc[mask, kw_cols].values.astype(np.float32)
sent = df.loc[mask, "sentiment"].values.reshape(-1, 1).astype(np.float32)

# Final feature matrix
#X = np.hstack([X_emb_labeled, kwf, sent])
X = np.hstack([X_emb_labeled])

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# X_train = emb_all[train_df.index]
# y_train = train_df["subtype_label"]

# X_test = emb_all[test_df.index]
# y_test = test_df["subtype_label"]

rf = RandomForestClassifier(
    n_estimators=300,
    max_depth=25,
    class_weight="balanced",
    random_state=42,
    n_jobs=-1
)

rf.fit(X_train, y_train)
preds_rf = rf.predict(X_test)

print("Random Forest")
print(classification_report(y_test, preds_rf))

import pandas as pd
from sklearn.metrics import f1_score, accuracy_score

results = []

for name, preds in [
    ("Logistic Regression", preds_lr),
    ("Linear SVM", preds_svm),
    ("Random Forest", preds_rf),
]:
    results.append({
        "Model": name,
        "Accuracy": accuracy_score(y_test, preds),
        "Macro F1": f1_score(y_test, preds, average="macro"),
        "Weighted F1": f1_score(y_test, preds, average="weighted")
    })

results_df = pd.DataFrame(results)
results_df

def compute_metrics(y_true, y_pred, model_name):
    acc = accuracy_score(y_true, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average="macro"
    )

    return {
        "model": model_name,
        "accuracy": acc,
        "precision_macro": precision,
        "recall_macro": recall,
        "f1_macro": f1
    }



metrics = []

# 1ï¸âƒ£ Logistic Regression
lr = LogisticRegression(max_iter=5000, class_weight="balanced")
lr.fit(X_train, y_train)
pred_lr = lr.predict(X_test)
metrics.append(compute_metrics(y_test, pred_lr, "Logistic Regression"))
joblib.dump(lr, "models/lr_model.pkl")

# 2ï¸âƒ£ Linear SVM
svm = LinearSVC(class_weight="balanced", max_iter=5000)
svm.fit(X_train, y_train)
pred_svm = svm.predict(X_test)
metrics.append(compute_metrics(y_test, pred_svm, "Linear SVM"))
joblib.dump(svm, "models/svm_model.pkl")

# 3ï¸âƒ£ Random Forest
rf = RandomForestClassifier(
    n_estimators=300,
    class_weight="balanced",
    random_state=42,
    n_jobs=-1
)
rf.fit(X_train, y_train)
pred_rf = rf.predict(X_test)
metrics.append(compute_metrics(y_test, pred_rf, "Random Forest"))
joblib.dump(rf, "models/rf_model.pkl")

metrics_df = pd.DataFrame(metrics)
metrics_df.to_csv("models/model_metrics.csv", index=False)

print(metrics_df)

import matplotlib.pyplot as plt

results_df.set_index("Model")[["Accuracy", "Macro F1"]].plot(
    kind="bar",
    figsize=(8,5)
)

plt.title("Model Comparison on Anxiety Subtype Classification")
plt.ylim(0, 1)
plt.grid(axis="y")
plt.show()

# =================================================================================
# Predict on full dataset using SAME features as training (SBERT ONLY)
# =================================================================================

# emb_all is already loaded from:
# emb_all = np.load("features/sbert_embeddings_all.npy")

# Predict using SVM (or Logistic Regression, whichever you want)
df["predicted_subtype"] = svm.predict(emb_all)

# Final subtype:
# - keep manual label if available
# - otherwise use predicted
df["final_subtype"] = np.where(
    df["subtype_label"] != "unknown",
    df["subtype_label"],
    df["predicted_subtype"]
)

# Save final dataset
df.to_csv("data/processed/reddit_with_predictions.csv", index=False)

print("Saved final dataset with SBERT-only predictions")

# =================================================================================
# Predict on full dataset and save final labels
# =================================================================================

# kwf_all = df[kw_cols].values.astype(np.float32)
# sent_all = df["sentiment"].values.reshape(-1,1).astype(np.float32)

# X_all = np.hstack([emb_all, kwf_all, sent_all])

# df["predicted_subtype"] = clf.predict(X_all)

# df["final_subtype"] = np.where(
#     df["subtype_label"] != "unknown",
#     df["subtype_label"],
#     df["predicted_subtype"]
# )

# df.to_csv("data/processed/reddit_with_predictions.csv", index=False)

# print("Saved final dataset")

print((df["predicted_subtype"] == "unknown").value_counts())

df["final_subtype"] = df["subtype_label"].where(
    df["subtype_label"] != "unknown",
    df["predicted_subtype"]
)

df_final = df[["post_id", "clean_text", "final_subtype"]]

df_final.to_csv(
    "data/processed/final_labels_only.csv",
    index=False
)

df_final.head(3)

import matplotlib.pyplot as plt

df["final_subtype"].value_counts().plot(kind="bar")
plt.title("Distribution of Anxiety Subtypes")
plt.xlabel("Subtype")
plt.ylabel("Number of Posts")
plt.show()

df["final_subtype"].value_counts(normalize=True).plot(
    kind="pie", autopct="%1.1f%%", figsize=(6,6)
)
plt.title("Subtype Proportion (%)")
plt.ylabel("")
plt.show()

plt.hist(df["sentiment"], bins=30)
plt.title("Sentiment Score Distribution")
plt.xlabel("Sentiment (-1 to 1)")
plt.ylabel("Count")
plt.show()

df["sentiment_bin"].value_counts().plot(kind="bar")
plt.title("Sentiment Category Distribution")
plt.xlabel("Sentiment Bin")
plt.ylabel("Count")
plt.show()

df.head(2)

import seaborn as sns

plt.figure(figsize=(8,5))
sns.boxplot(
    data=df,
    x="final_subtype",
    y="sentiment",
    showfliers=False
)
plt.title("Sentiment Distribution by Anxiety Subtype")
plt.show()

pivot = pd.crosstab(df["final_subtype"], df["sentiment_bin"])

sns.heatmap(pivot, annot=True, fmt="d", cmap="Blues")
plt.title("Subtype vs Sentiment Bin")
plt.show()

kw_cols = ["gad_kw_freq", "panic_kw_freq", "social_kw_freq"]

df.groupby("final_subtype")[kw_cols].mean().plot(kind="bar")
plt.title("Average Keyword Frequency per Subtype")
plt.ylabel("Average Count")
plt.show()

df[kw_cols].sum().plot(kind="pie", autopct="%1.1f%%")
plt.title("Overall Keyword Contribution")
plt.ylabel("")
plt.show()

from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, random_state=42)
emb_2d = tsne.fit_transform(emb_all)

plt.figure(figsize=(8,6))
sns.scatterplot(
    x=emb_2d[:,0],
    y=emb_2d[:,1],
    hue=df["final_subtype"],
    alpha=0.6
)
plt.title("Semantic Clustering of Posts (SBERT + t-SNE)")
plt.show()

plt.scatter(
    emb_2d[:,0],
    emb_2d[:,1],
    c=df["sentiment"],
    cmap="coolwarm",
    alpha=0.6
)
plt.colorbar(label="Sentiment")
plt.title("Sentiment Gradient over Semantic Space")
plt.show()

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, preds)

sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

df_report = pd.DataFrame(
    classification_report(y_test, preds, output_dict=True)
).transpose()

df_report.loc[
    ["generalized_anxiety", "panic_disorder", "social_anxiety"],
    ["precision", "recall", "f1-score"]
].plot(kind="bar", figsize=(8,5))

plt.title("Precision / Recall / F1 per Class")
plt.ylim(0, 1.05)
plt.grid(axis="y")
plt.show()

df["was_unknown"] = df["subtype_label"] == "unknown"

df["was_unknown"].value_counts().plot(kind="bar")
plt.title("Originally Unknown vs Labeled Posts")
plt.show()

cm_norm = cm.astype("float") / cm.sum(axis=1, keepdims=True)

plt.figure(figsize=(6,5))
sns.heatmap(
    cm_norm,
    annot=True,
    fmt=".2f",
    xticklabels=labels,
    yticklabels=labels,
    cmap="Blues"
)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Normalized Confusion Matrix")
plt.show()

!pip install streamlit pyngrok umap-learn seaborn scikit-learn

from pyngrok import ngrok

ngrok.set_auth_token("350pZUNQdUToiFtYTfEgaRI00bY_7EhfZh4N9gfSMk4baZPCr")

#!streamlit run app.py &>/content/logs.txt &

public_url = ngrok.connect(8501)
print("ðŸš€ Streamlit app is live at:", public_url)

!streamlit run app.py --server.port 8501 --server.address 0.0.0.0